{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMyaAJKkXiOsVu2HInPqPL7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KIl0qaV7FN__","executionInfo":{"status":"ok","timestamp":1679306288899,"user_tz":-330,"elapsed":25273,"user":{"displayName":"PULKIT AGARWAL","userId":"14149031172643726642"}},"outputId":"d172add4-07fd-4795-fc4b-1d5786a15f69"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["%cd drive/MyDrive/Colab\\ Notebooks/Minor\\ Project\\ Sem\\ 6/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yWLm9pzJFhrg","executionInfo":{"status":"ok","timestamp":1679306288900,"user_tz":-330,"elapsed":8,"user":{"displayName":"PULKIT AGARWAL","userId":"14149031172643726642"}},"outputId":"d4521c42-8098-4e38-e6cf-977ccc8925d5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/Minor Project Sem 6\n"]}]},{"cell_type":"markdown","source":["# Import all packages"],"metadata":{"id":"b-o7SDM0GI8s"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","\n","import nltk\n","import spacy\n","from nltk.tokenize import sent_tokenize, word_tokenize, regexp_tokenize\n","from nltk.corpus import stopwords\n","\n","from ast import literal_eval # to convert array string to array\n","from IPython.display import clear_output # to clear the large outputs\n","\n","import re\n","import string\n","import operator\n","from math import log2"],"metadata":{"id":"Vx3PPIHMFk1n","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679306305599,"user_tz":-330,"elapsed":16703,"user":{"displayName":"PULKIT AGARWAL","userId":"14149031172643726642"}},"outputId":"b1594a3e-de29-4314-bde5-85e188f990b7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n","  warnings.warn(\"Can't initialize NVML\")\n"]}]},{"cell_type":"code","source":["!python -m spacy download en_core_web_lg\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('averaged_perceptron_tagger')\n","clear_output()"],"metadata":{"id":"aRkEWm3kGNvc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install 'scipy>=1.8'\n","!pip install 'networkx<2.7'\n","clear_output()"],"metadata":{"id":"0h_2gA5YGaMA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install Keras-Preprocessing\n","from sklearn.metrics.pairwise import cosine_similarity\n","from keras_preprocessing.sequence import pad_sequences\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","clear_output()"],"metadata":{"id":"2vhcbLVAIgq4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install pyspellchecker\n","from spellchecker import SpellChecker\n","from collections import Counter\n","clear_output()"],"metadata":{"id":"C_xU3pTYLqK0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Read data from csv file"],"metadata":{"id":"C0IajQSqG-6n"}},{"cell_type":"code","source":["df = pd.read_csv('duc2002finaldataset_0.csv')\n","df = df.iloc[:-1, :]\n","articles = df.drop('Summary', axis=1)\n","articles.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"HOwKYCwHGuow","executionInfo":{"status":"ok","timestamp":1679307280500,"user_tz":-330,"elapsed":11,"user":{"displayName":"PULKIT AGARWAL","userId":"14149031172643726642"}},"outputId":"b04a9ba6-df70-4c5f-a4ef-dbacbef04569"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                             Article\n","0  ['On the day of the Big Event, Ladbroke, the l...\n","1  [\"Australian novelist Peter Carey was awarded ...\n","2  [\"Six novels have been nominated for the Booke...\n","3  [\"Japanese writer Kazuo Ishiguro won the 1989 ...\n","4  [\"The Booker Prize is Britain's literary event..."],"text/html":["\n","  <div id=\"df-f1860472-afeb-4dda-a5f3-9e704bb8c399\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Article</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>['On the day of the Big Event, Ladbroke, the l...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>[\"Australian novelist Peter Carey was awarded ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>[\"Six novels have been nominated for the Booke...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>[\"Japanese writer Kazuo Ishiguro won the 1989 ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>[\"The Booker Prize is Britain's literary event...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f1860472-afeb-4dda-a5f3-9e704bb8c399')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-f1860472-afeb-4dda-a5f3-9e704bb8c399 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-f1860472-afeb-4dda-a5f3-9e704bb8c399');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":38}]},{"cell_type":"markdown","source":["# Create functions for stop words removal and feature extractions"],"metadata":{"id":"r7qBJRWqHNAO"}},{"cell_type":"code","source":["sw = stopwords.words('english') \n","\n","def remove_stopwords(article):\n","  filtered_article = []\n","  for sen in article:\n","    words = word_tokenize(sen)\n","    filtered_article.append(' '.join({w.lower() for w in words if w.isalpha() and w.lower() not in sw}))\n","    # print(filtered_article[-1])\n","  return filtered_article"],"metadata":{"id":"d8juDgaPHFoq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# give a number to each sentance in article\n","def sentence_num(story):\n","    s=[]\n","    for i in range(len(story)):\n","        s.append(\"S\"+ str(i))\n","    return s"],"metadata":{"id":"WjmySftmJpUq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#program to sentence position\n","def sentenceposition(story):\n","    sentenceLen = len(story)\n","    sentence_position = []\n","\n","    for i in range(0, sentenceLen):\n","        sent_pos = round(((sentenceLen - i)/sentenceLen),2)\n","        sentence_position.append(sent_pos)\n","\n","    return sentence_position             #sentence_position = (sentenceLen - i) / sentenceLen"],"metadata":{"id":"dmjTL6zw7C2X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def open_relation(article):\n","  length = []\n","  for sentence in article:\n","    word_tokens = word_tokenize(sentence)\n","    filtered_words = [word for word in word_tokens if word.lower() not in stopwords.words('english') and len(word)>1]\n","    length.append(len(filtered_words))\n","  length = np.array(length)\n","  max_len = max(length)\n","  length = length / max_len\n","  return length"],"metadata":{"id":"lmHbwMbVHngg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#program to count sentence length of story\n","def sentencelength(story):\n","    story_len = len(story)\n","    sentence_count = []\n","    sentence_length = []\n","    max_word_count = 0\n","\n","    for i in range(0, story_len):\n","        # using regex (findall())\n","        # to count words in string\n","        res = len(re.findall(r'\\w+', story[i]))\n","        sentence_count.append(res)\n","\n","        #word count of each sentence\n","        #print(res)\n","\n","        if(res > max_word_count):\n","            max_word_count = res\n","\n","\n","    for j in range(0,len(sentence_count)):\n","        sentenceLen = round(sentence_count[j] / max_word_count,2)\n","        sentence_length.append(sentenceLen)\n","    \n","    return sentence_length"],"metadata":{"id":"hJz2tQ0YKHw0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#program to count numeric data in sentence of story\n","def numericdata(story):\n","    numeric_data = []\n","    for i in range(0, len(story)):\n","        # using regex (findall())\n","        # to count words in string\n","        words_count = len(re.findall(r'\\w+', story[i]))\n","        #print(res)\n","        pattern = '[0-9]+'\n","        numeric_count = len(re.findall(pattern, story[i]))\n","        #print(numeric_count)\n","        result = 0\n","        if(words_count != 0):\n","            result = result + numeric_count/words_count\n","        numeric_data.append(round(result,2))\n","    return numeric_data"],"metadata":{"id":"Cuff9XZIKUuN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#program to find number of named entity in each sentence\n","def NamedEntity(story):\n","    NER = spacy.load(\"en_core_web_sm\", disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"])\n","    #creating a list to store the number of named entity in each sentence\n","    NamedEntity_=[]\n","    for i in range(len(story)):\n","        text= NER(story[i])\n","        #appending the number of named entity in each sentence to the list\n","        NamedEntity_.append(len(text.ents))\n","    if(max(NamedEntity_) != 0):\n","        NamedEntity_=[round(i/max(NamedEntity_),2) for i in NamedEntity_]\n","    \n","    return NamedEntity_"],"metadata":{"id":"_pl0P9e9K8NG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Program to count PUNCTUATION MARKS\n","def specialcharecters(story):\n","    punctuation=[]\n","    for i in range(len(story)): \n","        count = 0 \n","        for j in range(len(story[i])):  \n","            if story[i][j] in string.punctuation:\n","                count = count + 1    \n","        punctuation.append(count)\n","    \n","    if(max(punctuation) != 0):\n","        punctuation=[round(i/max(punctuation),2) for i in punctuation]\n","    \n","    return punctuation"],"metadata":{"id":"rWbXbexzK-vW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# def thematicwords(story):\n","#     data = remove_stopwords(story)\n","#     frequency = {}\n","#     match_pattern = re.findall(r'\\b[a-z]{3,15}\\b', str(data).lower())\n","#     for word in match_pattern:\n","#         count = frequency.get(word,0)\n","#         frequency[word] = count + 1\n","#     length= len(frequency)//4\n","#     freq_sort=sorted(frequency.items(), key=lambda x: x[1], reverse=True)\n","#     first_data = list(map(operator.itemgetter(0), freq_sort))\n","#     thematic_words = first_data[:length+1]\n","#     tw=[]\n","#     for i in range(len(data)):\n","#         count=0\n","#         tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n","#         words = tokenizer.tokenize(data[i])\n","#         for j in range(len(words)):\n","#             if(words[j] in thematic_words):\n","#                 #print(words[j])\n","#                 count = count + 1\n","#         tw.append(count)  \n","#     if(max(tw) != 0):\n","#         thematic_words = [round(i/max(tw),2) for i in tw]\n","#     return thematic_words"],"metadata":{"id":"3hs-vpmJLLoF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#count no of uppercases \n","def Uppercase(story):\n","    UpperCase = []\n","    for i in range(0, len(story)):\n","        countUpperCase = 0\n","        token = regexp_tokenize(story[i], \"[\\w']+\")\n","        for j in token:\n","            if len(j) != 1 and j.isupper()==True:\n","                countUpperCase += 1\n","        UpperCase.append(countUpperCase)\n","    if(max(UpperCase) != 0):\n","            UpperCase=[round(i/max(UpperCase),2) for i in UpperCase]\n","    \n","    return UpperCase"],"metadata":{"id":"su5_sTUpLWMT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def entropy(story):\n","    for i in range(len(story)):\n","        story[i] = story[i].lower()\n","    for i in range(len(story)):\n","        for character in string.punctuation:\n","             story[i] = story[i].replace(character, '')\n","    data = remove_stopwords(story)\n","    def counting(elements):\n","        # check if each word has '.' at its last. If so then ignore '.'\n","        if elements[-1] == '.':\n","            elements = elements[0:len(elements) - 1]\n","\n","        # if there exists a key as \"elements\" then simply\n","        # increase its value.\n","        if elements in dictionary:\n","            dictionary[elements] += 1\n","\n","        # if the dictionary does not have the key as \"elements\" \n","        # then create a key \"elements\" and assign its value to 1.\n","        else:\n","            dictionary.update({elements: 1})\n","    totalCount = []\n","    for Sentence in data:\n","        dictionary = {}\n","        wordCount = []\n","        lst = Sentence.split()\n","        for elements in lst:\n","            counting(elements)\n","        for allKeys in dictionary:\n","            wordCount.append(dictionary[allKeys])\n","#             print (\"Frequency of \", allKeys, end = \" \")\n","#             print (\":\", end = \" \")\n","#             print (dictionary[allKeys], end = \" \")\n","#             print(\"-----------------\") \n","        totalCount.append(wordCount)\n","    lengthSentence = []    \n","    for i in range(0, len(data)):\n","        count = len(data[i].split())\n","        lengthSentence.append(count)\n","    def entropyCalculation(senList):\n","        entropy = 0\n","        i = 0\n","        length = lengthSentence[i]\n","        for freq in senList:\n","            if freq == 0:\n","              freq = 1\n","            if length == 0:\n","              length = 1\n","            prob = round(freq/length, 2)\n","            if prob == 0:\n","              prob = 0.001\n","            #print(-(prob * log2(prob)))\n","            entropy += -(prob * log2(prob))\n","            #print(entropy, \" \")\n","        return entropy\n","    entropyTotal = []\n","    for i in range(0, len(totalCount)):\n","        #print(totalCount[i])\n","        ent = entropyCalculation(totalCount[i])\n","        entropyTotal.append(round(ent,2))    \n","    if(max(entropyTotal) != 0):\n","        entropyTotal=[round(i/max(entropyTotal),2) for i in entropyTotal]\n","    \n","    return entropyTotal"],"metadata":{"id":"EhnWb_yZLXGR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Function to find incorrect words\n","#incorrect words\n","# find those words that may be misspelled\n","\n","def incorrect(story):\n","    incorrectWord = []\n","    spell = SpellChecker()\n","    for i in range(len(story)):\n","        for character in string.punctuation:\n","             story[i] = story[i].replace(character, '')\n","    \n","    for i in range(0, len(story)):\n","        l = story[i].split()\n","        #print(l)\n","        misspelled = spell.unknown(l)\n","        count = 0\n","        for word in misspelled:\n","            count = count + 1\n","        incorrectWord.append(count)\n","    if(max(incorrectWord) != 0):\n","        incorrectWord=[round(i/max(incorrectWord),2) for i in incorrectWord]\n","    \n","    return incorrectWord"],"metadata":{"id":"jYhkQtZoLXEF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Finding and updating Parts Of Speech (POS Tags)\n","\n","def postags(story):\n","    Postags=[]\n","    postags_ct = []\n","    for i in range(len(story)):\n","        ct = 0\n","        #tokenize the words in the text\n","        tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n","        tokens = tokenizer.tokenize(story[i])\n","        #assign POS tags to each words\n","        pos = nltk.pos_tag(tokens)\n","        #Count the POS tags\n","        the_count = dict(Counter(tag for _, tag in pos))\n","        #appending the count of each pos tags in a sentence to a list\n","        Postags.append(the_count)\n","        keys = the_count.keys()\n","        #adding nouns and verbs together under pos category\n","        for i in keys:\n","            if(i == \"NNP\" or i ==\"NNPS\" or i ==\"NN\" or i ==\"NNS\" or i ==\"VB\" or i ==\"VBD\" or i ==\"VBG\" or i ==\"VBN\" or i ==\"VBP\" or i ==\"VBZ\"):\n","                ct += the_count[i] \n","        postags_ct.append(ct)\n","    if(max(postags_ct) != 0):\n","        postags_ct=[round(i/max(postags_ct),2) for i in postags_ct]\n","    return postags_ct"],"metadata":{"id":"SOBYiRSJLXBf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"_6x1bKFZLW5_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"juMkJ9GlLW37"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Use this instead of manually finding tf_isf\n","def tf_isf(story):\n","    vectorizer = TfidfVectorizer()\n","    vectors = vectorizer.fit_transform(story)\n","    #feature_names = vectorizer.get_feature_names()\n","    dense = vectors.todense()\n","    denselist = dense.tolist()\n","    scores=[]\n","    for i in range(len(denselist)):\n","        score=0\n","        for j in range(len(denselist[i])):\n","            score+=denselist[i][j]\n","        scores.append(score)\n","    if(max(scores) != 0):\n","        scores=[round(i/max(scores),2) for i in scores]\n","    return scores"],"metadata":{"id":"VeO0tTl-MMED"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"b8aKb47iMMpc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"5yFxbtoZMMnY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# def cosine_similarity(tf_isf_mat):\n","#   # Pad vectors to have the same length\n","#   padded_vectors = np.array(pad_sequences(tf_isf_mat, padding='post', value=0, dtype='float'))\n","\n","#   # Compute cosine similarity between each pair of vectors\n","#   cosine_similarity_mat = cosine_similarity(padded_vectors)\n","\n","#   # Compute cosine similarity between each pair of vectors\n","\n","#   cosine_similarity_mat = np.array(cosine_similarity_mat)\n","\n","#   # Convert similarity values to distances\n","#   distances = 1 - cosine_similarity_mat\n","#   distances = np.round(distances.clip(min=0), 2)\n","#   return distances"],"metadata":{"id":"0ze-pYOXIX7J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# def aggregation_similarity(cosine_similarity_mat):\n","#   return [sum(i) for i in cosine_similarity_mat]"],"metadata":{"id":"hSl1nVfGMptg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Use this instead of finding similarity manually\n","def sentence_similarity(story):\n","    Tfidf_vect = TfidfVectorizer()\n","    vector_matrix = Tfidf_vect.fit_transform(story)\n","    #tokens = Tfidf_vect.get_feature_names()\n","    cosine_similarity_matrix = cosine_similarity(vector_matrix)\n","    cosines=[]\n","    for i in range(len(cosine_similarity_matrix)):\n","        cos=0\n","        for j in range(len(cosine_similarity_matrix[i])):\n","            cos= cos + cosine_similarity_matrix[i][j]\n","        cosines.append(cos)\n","    if(max(cosines) != 0):\n","        cosines=[round(i/max(cosines),2) for i in cosines]\n","    return cosines"],"metadata":{"id":"Xy-3DZHGMQ4B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"JosMg-7rM1gk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def title_feature(story,title):\n","    title_features = []\n","    title_words = word_tokenize(title)\n","    length_title = len(title_words)\n","    for i in range(len(story)):\n","        score = 0\n","        sentence_words = word_tokenize(story[i])\n","        for word in sentence_words:\n","            if word in title_words:\n","                score += 1\n","        title_features.append(score)\n","    title_features=[i/length_title for i in title_features]\n","    return title_features"],"metadata":{"id":"-dLECBJnM1eG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"A2uUzV-AR-zA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from scipy.sparse import coo_matrix\n","import networkx as nx\n","import matplotlib.pyplot as plt\n","\n","def busy_path(article):\n","  Tfidf_vect = TfidfVectorizer()\n","  vector_matrix = Tfidf_vect.fit_transform(story)\n","  #tokens = Tfidf_vect.get_feature_names()\n","  cosine_similarity_matrix = cosine_similarity(vector_matrix)\n","  distances = 1 - cosine_similarity_matrix\n","  distances = np.round(distances.clip(min=0), 2)\n","\n","  m = distances.copy()\n","  for i in range(len(m)):\n","    for j in range(len(m[i])):\n","      if m[i][j] < 0.95:\n","        m[i][j] = 0\n","    \n","  sparse_matrix = coo_matrix(m) # <18x18 sparse matrix of type '<class 'numpy.float64'>'\twith 306 stored elements in COOrdinate format> # Means 306 nodes are there\n","  # Create graph\n","  G = nx.from_scipy_sparse_matrix(sparse_matrix)\n","  bushy_path_mat = [value for i, value in G.degree]\n","  return bushy_path_mat"],"metadata":{"id":"JReBiys0TYw_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def text_rank(article):\n","  Tfidf_vect = TfidfVectorizer()\n","  vector_matrix = Tfidf_vect.fit_transform(story)\n","  #tokens = Tfidf_vect.get_feature_names()\n","  cosine_similarity_matrix = cosine_similarity(vector_matrix)\n","  distances = 1 - cosine_similarity_matrix\n","  distances = np.round(distances.clip(min=0), 2)\n","\n","  # Adjecency list calucate\n","  m = [[] for _ in range(len(distances))]\n","\n","  for i in range(len(distances)):\n","    for j in range(len(distances[i])):\n","      if distances[i][j] >= 0.95:\n","        m[i].append(j)\n","  \n","  # Page rank calculation using custom furmula\n","  n = len(m)\n","  d, it = .85, 100\n","  rank = [1 for _ in range(n)]\n","\n","  # Calculate rank for each iteration\n","  for _ in range(it):\n","    for i in range(n):\n","      rank[i] = (1-d) + d * sum([rank[x]/len(m[x]) for x in m[i]])\n","\n","  return rank"],"metadata":{"id":"hIfwRSDxTbZs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Cq85s2JLTbXj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Convert all the features into a csv file for each article"],"metadata":{"id":"2OjpPR2eR_5c"}},{"cell_type":"code","source":["all_data = None\n","\n","# for i in range(2):\n","for i in range(len(articles['Article'])):\n","  print(i)\n","  story = literal_eval(articles['Article'][i])\n","  df = pd.DataFrame({\n","          'File Number ': \"F\" + str(i),\n","          'Sentence Number': sentence_num(story),\n","          'Sentence length': sentencelength(story),\n","          'Sentence Position': sentenceposition(story),\n","          'Numeric Data': numericdata(story),\n","          'Named Entity': NamedEntity(story),\n","          'Special Charecters': specialcharecters(story),\n","          # 'Thematic Words': thematicwords(story),\n","          'Upper Case': Uppercase(story),\n","          'Entropy': entropy(story),\n","          'Incorrect Word': incorrect(story),\n","          'POS Tags': postags(story),\n","          'Term Weight': tf_isf(story),\n","          'Cosine Similarity': sentence_similarity(story), # aggregation similarity\n","          # 'Title Feature': title_feature(story, title), # what is title here\n","          'Bushy Path': busy_path(story),\n","          'Text Rank': text_rank(story)\n","      })\n","\n","  if all_data is None:\n","    all_data = df\n","  else:\n","    all_data = pd.concat([all_data, df], ignore_index=True)\n","  if i%20 == 0:\n","    clear_output()"],"metadata":{"id":"bnc1xji3JEbl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["all_data.head()\n","# all_data.shape\n","# all_data.tail()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":267},"id":"OXijpJM8WECW","executionInfo":{"status":"ok","timestamp":1679307143613,"user_tz":-330,"elapsed":932,"user":{"displayName":"PULKIT AGARWAL","userId":"14149031172643726642"}},"outputId":"acb2bb74-2ac7-431e-d0ef-ede8881f5f06"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["  File Number  Sentence Number  Sentence length  Sentence Position  \\\n","0           F0              S0             0.20               1.00   \n","1           F0              S1             0.28               0.98   \n","2           F0              S2             0.18               0.96   \n","3           F0              S3             0.23               0.94   \n","4           F0              S4             0.20               0.92   \n","\n","   Numeric Data  Named Entity  Special Charecters  Upper Case  Entropy  \\\n","0          0.00          0.10                0.06         0.0     0.29   \n","1          0.35          0.28                0.21         0.0     0.26   \n","2          0.13          0.14                0.06         0.0     0.21   \n","3          0.00          0.00                0.05         0.0     0.24   \n","4          0.06          0.07                0.02         0.0     0.21   \n","\n","   Incorrect Word  POS Tags  Term Weight  Cosine Similarity  Busy Path  \\\n","0            0.09      0.18         0.59               0.79         25   \n","1            0.27      0.29         0.68               0.47         42   \n","2            0.09      0.13         0.58               0.49         41   \n","3            0.00      0.18         0.65               0.60         33   \n","4            0.00      0.16         0.62               0.60         30   \n","\n","   Text Rank  \n","0   0.795211  \n","1   1.264775  \n","2   1.238773  \n","3   1.010622  \n","4   0.930889  "],"text/html":["\n","  <div id=\"df-487696dc-c07b-4492-b45c-5b09e89f634b\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>File Number</th>\n","      <th>Sentence Number</th>\n","      <th>Sentence length</th>\n","      <th>Sentence Position</th>\n","      <th>Numeric Data</th>\n","      <th>Named Entity</th>\n","      <th>Special Charecters</th>\n","      <th>Upper Case</th>\n","      <th>Entropy</th>\n","      <th>Incorrect Word</th>\n","      <th>POS Tags</th>\n","      <th>Term Weight</th>\n","      <th>Cosine Similarity</th>\n","      <th>Busy Path</th>\n","      <th>Text Rank</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>F0</td>\n","      <td>S0</td>\n","      <td>0.20</td>\n","      <td>1.00</td>\n","      <td>0.00</td>\n","      <td>0.10</td>\n","      <td>0.06</td>\n","      <td>0.0</td>\n","      <td>0.29</td>\n","      <td>0.09</td>\n","      <td>0.18</td>\n","      <td>0.59</td>\n","      <td>0.79</td>\n","      <td>25</td>\n","      <td>0.795211</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>F0</td>\n","      <td>S1</td>\n","      <td>0.28</td>\n","      <td>0.98</td>\n","      <td>0.35</td>\n","      <td>0.28</td>\n","      <td>0.21</td>\n","      <td>0.0</td>\n","      <td>0.26</td>\n","      <td>0.27</td>\n","      <td>0.29</td>\n","      <td>0.68</td>\n","      <td>0.47</td>\n","      <td>42</td>\n","      <td>1.264775</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>F0</td>\n","      <td>S2</td>\n","      <td>0.18</td>\n","      <td>0.96</td>\n","      <td>0.13</td>\n","      <td>0.14</td>\n","      <td>0.06</td>\n","      <td>0.0</td>\n","      <td>0.21</td>\n","      <td>0.09</td>\n","      <td>0.13</td>\n","      <td>0.58</td>\n","      <td>0.49</td>\n","      <td>41</td>\n","      <td>1.238773</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>F0</td>\n","      <td>S3</td>\n","      <td>0.23</td>\n","      <td>0.94</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.05</td>\n","      <td>0.0</td>\n","      <td>0.24</td>\n","      <td>0.00</td>\n","      <td>0.18</td>\n","      <td>0.65</td>\n","      <td>0.60</td>\n","      <td>33</td>\n","      <td>1.010622</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>F0</td>\n","      <td>S4</td>\n","      <td>0.20</td>\n","      <td>0.92</td>\n","      <td>0.06</td>\n","      <td>0.07</td>\n","      <td>0.02</td>\n","      <td>0.0</td>\n","      <td>0.21</td>\n","      <td>0.00</td>\n","      <td>0.16</td>\n","      <td>0.62</td>\n","      <td>0.60</td>\n","      <td>30</td>\n","      <td>0.930889</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-487696dc-c07b-4492-b45c-5b09e89f634b')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-487696dc-c07b-4492-b45c-5b09e89f634b button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-487696dc-c07b-4492-b45c-5b09e89f634b');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":31}]},{"cell_type":"code","source":["all_data.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AyTu-EJGgY4a","executionInfo":{"status":"ok","timestamp":1679307151059,"user_tz":-330,"elapsed":435,"user":{"displayName":"PULKIT AGARWAL","userId":"14149031172643726642"}},"outputId":"a1b30c0f-1714-4b68-efb9-279285a7aefb"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(15672, 15)"]},"metadata":{},"execution_count":32}]},{"cell_type":"code","source":["all_data.to_csv('features/features_for_all_articles.csv', index=False)"],"metadata":{"id":"vTeykhtAUi_K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"zbujp3-BVQat"},"execution_count":null,"outputs":[]}]}